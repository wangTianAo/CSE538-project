[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "vocab_size": 50261
}

Model has a total of 124442880 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=100, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=30.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=5000, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 29
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 249990
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "vocab_size": 50261
}

Model has a total of 124442880 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=100, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=30.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=5000, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 29
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 249990
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "vocab_size": 50261
}

Model has a total of 124442880 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=100, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=30.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=5000, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 29
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 249990
  steps: 100  ppl: 3.3423
  steps: 200  ppl: 2.2919
  steps: 300  ppl: 2.1731
  steps: 400  ppl: 2.1033
  steps: 500  ppl: 2.0383
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "vocab_size": 50261
}

Model has a total of 124442880 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=100, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=5.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=24, per_gpu_train_batch_size=12, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=500, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 4
  Instantaneous batch size per GPU = 12
  Total train batch size (w. parallel, distributed & accumulation) = 24
  Gradient Accumulation steps = 2
  Total optimization steps = 20830
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "vocab_size": 50261
}

Model has a total of 124442880 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=100, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=5.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=24, per_gpu_train_batch_size=12, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=100, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 4
  Instantaneous batch size per GPU = 12
  Total train batch size (w. parallel, distributed & accumulation) = 24
  Gradient Accumulation steps = 2
  Total optimization steps = 20830
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "vocab_size": 50261
}

Model has a total of 124442880 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=100, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=5.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=100, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 4
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 41665
  steps: 100  ppl: 3.3422
Data size: 2000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "vocab_size": 50261
}

Model has a total of 124442880 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=100, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=5.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=100, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 4
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 41665
  steps: 100  ppl: 3.3422
Data size: 2000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "vocab_size": 50261
}

Model has a total of 124442880 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=10, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=5.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=100, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 4
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 41665
  steps: 10  ppl: 14.4686
  steps: 20  ppl: 4.3481
  steps: 30  ppl: 3.2251
  steps: 40  ppl: 2.864
  steps: 50  ppl: 2.6762
  steps: 60  ppl: 2.7113
  steps: 70  ppl: 2.5549
  steps: 80  ppl: 2.4675
  steps: 90  ppl: 2.6168
  steps: 100  ppl: 2.5002
Data size: 2000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "vocab_size": 50261
}

Model has a total of 124442880 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=10, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=5.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=100, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 4
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 41665
  steps: 10  ppl: 14.4686
  steps: 20  ppl: 4.3481
  steps: 30  ppl: 3.2251
  steps: 40  ppl: 2.864
  steps: 50  ppl: 2.6762
  steps: 60  ppl: 2.7113
  steps: 70  ppl: 2.5549
  steps: 80  ppl: 2.4675
  steps: 90  ppl: 2.6168
  steps: 100  ppl: 2.5002
Data size: 2000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "vocab_size": 50261
}

Model has a total of 124442880 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=10, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=5.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=50, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 4
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 41665
  steps: 10  ppl: 14.4686
  steps: 20  ppl: 4.3481
  steps: 30  ppl: 3.2251
  steps: 40  ppl: 2.864
  steps: 50  ppl: 2.6762
Data size: 2000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "vocab_size": 50261
}

Model has a total of 124442880 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=10, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=5.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=50, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 4
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 41665
  steps: 10  ppl: 14.4686
  steps: 20  ppl: 4.3481
  steps: 30  ppl: 3.2251
  steps: 40  ppl: 2.864
  steps: 50  ppl: 2.6762
Data size: 2000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "vocab_size": 50261
}

Model has a total of 124442880 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=10, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=5.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=50, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 4
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 41665
  steps: 10  ppl: 14.4686
  steps: 20  ppl: 4.3481
  steps: 30  ppl: 3.2251
  steps: 40  ppl: 2.864
  steps: 50  ppl: 2.6762
Data size: 2000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "vocab_size": 50261
}

Model has a total of 124442880 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=10, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=5.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=50, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 4
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 41665
  steps: 10  ppl: 14.4686
  steps: 20  ppl: 4.3481
  steps: 30  ppl: 3.2251
  steps: 40  ppl: 2.864
  steps: 50  ppl: 2.6762
Data size: 2000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "vocab_size": 50261
}

Model has a total of 124442880 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=10, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=5.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=50, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 4
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 41665
  steps: 10  ppl: 12.8554
  steps: 20  ppl: 4.2073
  steps: 30  ppl: 3.1762
  steps: 40  ppl: 2.87
  steps: 50  ppl: 2.7046
Data size: 2000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "vocab_size": 50261
}

Model has a total of 124442880 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=10, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=5.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=50, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 4
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 41665
  steps: 10  ppl: 12.8554
  steps: 20  ppl: 4.2073
  steps: 30  ppl: 3.1762
  steps: 40  ppl: 2.87
  steps: 50  ppl: 2.7046
Data size: 2000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "vocab_size": 50261
}

Model has a total of 124442880 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=10, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=5.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=50, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 4
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 41665
  steps: 10  ppl: 12.8554
  steps: 20  ppl: 4.2073
  steps: 30  ppl: 3.1762
  steps: 40  ppl: 2.87
  steps: 50  ppl: 2.7046
Data size: 2000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "vocab_size": 50261
}

Model has a total of 124442880 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=10, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=5.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=50, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 4
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 41665
  steps: 10  ppl: 12.8554
  steps: 20  ppl: 4.2073
  steps: 30  ppl: 3.1762
  steps: 40  ppl: 2.87
  steps: 50  ppl: 2.7046
Data size: 2000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "vocab_size": 50261
}

Model has a total of 124442880 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=10, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=5.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=50, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 4
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 41665
  steps: 10  ppl: 12.8554
  steps: 20  ppl: 4.2073
  steps: 30  ppl: 3.1762
  steps: 40  ppl: 2.87
  steps: 50  ppl: 2.7046
Data size: 2000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "vocab_size": 50261
}

Model has a total of 124442880 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=10, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=5.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=50, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 4
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 41665
  steps: 10  ppl: 12.8554
  steps: 20  ppl: 4.2073
  steps: 30  ppl: 3.1762
  steps: 40  ppl: 2.87
  steps: 50  ppl: 2.7046
Data size: 2000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "vocab_size": 50261
}

Model has a total of 124442880 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=10, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=5.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=50, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 4
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 41665
  steps: 10  ppl: 12.8554
  steps: 20  ppl: 4.2073
  steps: 30  ppl: 3.1762
  steps: 40  ppl: 2.87
  steps: 50  ppl: 2.7046
Data size: 2000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "vocab_size": 50261
}

Model has a total of 124442880 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=10, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=5.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=50, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 4
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 41665
  steps: 10  ppl: 12.8554
  steps: 20  ppl: 4.2073
  steps: 30  ppl: 3.1762
  steps: 40  ppl: 2.87
  steps: 50  ppl: 2.7046
Data size: 2000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "vocab_size": 50261
}

Model has a total of 124442880 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=10, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=5.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=50, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 4
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 41665
  steps: 10  ppl: 12.8554
  steps: 20  ppl: 4.2073
  steps: 30  ppl: 3.1762
  steps: 40  ppl: 2.87
  steps: 50  ppl: 2.7046
Data size: 2000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "vocab_size": 50261
}

Model has a total of 124442880 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=10, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=5.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=50, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 4
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 41665
  steps: 10  ppl: 12.8554
  steps: 20  ppl: 4.2073
  steps: 30  ppl: 3.1762
  steps: 40  ppl: 2.87
  steps: 50  ppl: 2.7046
Data size: 2000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "vocab_size": 50261
}

Model has a total of 124442880 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=10, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=5.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=50, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 4
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 41665
  steps: 10  ppl: 12.8554
  steps: 20  ppl: 4.2073
  steps: 30  ppl: 3.1762
  steps: 40  ppl: 2.87
  steps: 50  ppl: 2.7046
Data size: 2000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "vocab_size": 50261
}

Model has a total of 124442880 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=10, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=5.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=50, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 4
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 41665
  steps: 10  ppl: 12.8554
  steps: 20  ppl: 4.2073
  steps: 30  ppl: 3.1762
  steps: 40  ppl: 2.87
  steps: 50  ppl: 2.7046
Data size: 2000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "vocab_size": 50261
}

Model has a total of 124442880 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=10, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=5.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=50, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 4
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 41665
  steps: 10  ppl: 12.8554
  steps: 20  ppl: 4.2073
  steps: 30  ppl: 3.1762
  steps: 40  ppl: 2.87
  steps: 50  ppl: 2.7046
Data size: 2000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "vocab_size": 50261
}

Model has a total of 124442880 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=10, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=5.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=50, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 4
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 41665
  steps: 10  ppl: 12.8554
  steps: 20  ppl: 4.2073
  steps: 30  ppl: 3.1762
  steps: 40  ppl: 2.87
  steps: 50  ppl: 2.7046
Data size: 2000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "vocab_size": 50261
}

Model has a total of 124442880 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=10, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=5.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=50, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 4
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 41665
  steps: 10  ppl: 12.8554
  steps: 20  ppl: 4.2073
  steps: 30  ppl: 3.1762
  steps: 40  ppl: 2.87
  steps: 50  ppl: 2.7046
Data size: 2000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "vocab_size": 50261
}

Model has a total of 124442880 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=10, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=5.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=50, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 4
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 41665
  steps: 10  ppl: 12.8554
  steps: 20  ppl: 4.2073
  steps: 30  ppl: 3.1762
  steps: 40  ppl: 2.87
  steps: 50  ppl: 2.7046
Data size: 2000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "vocab_size": 50261
}

Model has a total of 124442880 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=10, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=5.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=50, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 4
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 41665
  steps: 10  ppl: 12.8554
  steps: 20  ppl: 4.2073
  steps: 30  ppl: 3.1762
  steps: 40  ppl: 2.87
  steps: 50  ppl: 2.7046
Data size: 2000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "vocab_size": 50261
}

Model has a total of 124442880 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=10, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=5.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=50, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 4
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 41665
  steps: 10  ppl: 12.8554
  steps: 20  ppl: 4.2073
  steps: 30  ppl: 3.1762
  steps: 40  ppl: 2.87
  steps: 50  ppl: 2.7046
Data size: 2000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "vocab_size": 50261
}

Model has a total of 124442880 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=10, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=5.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=50, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 4
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 41665
  steps: 10  ppl: 12.8554
  steps: 20  ppl: 4.2073
  steps: 30  ppl: 3.1762
  steps: 40  ppl: 2.87
  steps: 50  ppl: 2.7046
Data size: 2000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "vocab_size": 50261
}

Model has a total of 124442880 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=10, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=5.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=50, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 4
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 41665
  steps: 10  ppl: 12.8554
  steps: 20  ppl: 4.2073
  steps: 30  ppl: 3.1762
  steps: 40  ppl: 2.87
  steps: 50  ppl: 2.7046
Data size: 2000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "vocab_size": 50261
}

Model has a total of 124442880 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=10, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=5.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=50, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 4
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 41665
  steps: 10  ppl: 12.8554
  steps: 20  ppl: 4.2073
  steps: 30  ppl: 3.1762
  steps: 40  ppl: 2.87
  steps: 50  ppl: 2.7046
Data size: 2000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "vocab_size": 50261
}

Model has a total of 124442880 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=10, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=5.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=10, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 4
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 41665
  steps: 10  ppl: 12.8554
Data size: 2000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "vocab_size": 50261
}

Model has a total of 124442880 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=10, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=5.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=10, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 4
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 41665
  steps: 10  ppl: 12.8554
Data size: 2000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "vocab_size": 50261
}

Model has a total of 124442880 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=10, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=5.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=10, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 4
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 41665
  steps: 10  ppl: 12.8554
Data size: 2000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "vocab_size": 50261
}

Model has a total of 124442880 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=10, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=5.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=10, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 4
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 41665
  steps: 10  ppl: 12.8554
Data size: 2000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "vocab_size": 50261
}

Model has a total of 124442880 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=10, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=5.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=10, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 4
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 41665
  steps: 10  ppl: 12.8554
Data size: 2000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "vocab_size": 50261
}

Model has a total of 124442880 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=10, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=5.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=10, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 4
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 41665
  steps: 10  ppl: 12.8554
Data size: 2000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "vocab_size": 50261
}

Model has a total of 124442880 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=10, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=5.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=10, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 4
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 41665
  steps: 10  ppl: 12.8554
Data size: 2000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "vocab_size": 50261
}

Model has a total of 124442880 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=10, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=5.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=10, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 4
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 41665
  steps: 10  ppl: 12.8554
Data size: 2000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "vocab_size": 50261
}

Model has a total of 124442880 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=10, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=5.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=10, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 4
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 41665
  steps: 10  ppl: 12.8554
Data size: 2000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "vocab_size": 50261
}

Model has a total of 124442880 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=10, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=5.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=10, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 4
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 41665
  steps: 10  ppl: 12.8554
Data size: 2000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "vocab_size": 50261
}

Model has a total of 124442880 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=10, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=5.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=10, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 4
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 41665
  steps: 10  ppl: 12.8554
Data size: 2000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "vocab_size": 50261
}

Model has a total of 124442880 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=10, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=5.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=10, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 4
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 41665
  steps: 10  ppl: 12.8554
Data size: 2000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "vocab_size": 50261
}

Model has a total of 124442880 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=10, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=5.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=10, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 4
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 41665
  steps: 10  ppl: 12.8554
Data size: 2000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "do_sample": false,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "eos_token_ids": 0,
  "finetuning_task": null,
  "gradient_checkpointing": false,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_epsilon": 1e-05,
  "length_penalty": 1.0,
  "max_length": 20,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "num_beams": 1,
  "num_labels": 2,
  "num_return_sequences": 1,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 50257,
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "use_bfloat16": false,
  "vocab_size": 50260
}

Model has a total of 124442112 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=10, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=5.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=10, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 4
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 41665
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "do_sample": false,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "eos_token_ids": 0,
  "finetuning_task": null,
  "gradient_checkpointing": false,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_epsilon": 1e-05,
  "length_penalty": 1.0,
  "max_length": 20,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "num_beams": 1,
  "num_labels": 2,
  "num_return_sequences": 1,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 50257,
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "use_bfloat16": false,
  "vocab_size": 50260
}

Model has a total of 124442112 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=10, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=5.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=10, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 4
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 41665
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "vocab_size": 50261
}

Model has a total of 124442880 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=10, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=5.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=10, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 4
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 41665
  steps: 10  ppl: 11.7332
Data size: 2000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
0 are done!
10 are done!
20 are done!
30 are done!
40 are done!
50 are done!
60 are done!
70 are done!
80 are done!
90 are done!
dev bleu: 4.31, dev EM: 0.0
best bleu updated. saved in ../save/concode/checkpoint-10-4.31
best bleu: 4.31
Saving model checkpoint to ../save/concode/checkpoint-10-4.31
Saving optimizer and scheduler states to ../save/concode/checkpoint-last
  steps: 20  ppl: 4.2946
Data size: 2000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
0 are done!
10 are done!
20 are done!
30 are done!
40 are done!
50 are done!
60 are done!
70 are done!
reload model from ../save/concode/checkpoint-last, resume from 1 epoch
[50259, 23748, 995, 50257, 50258]
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "vocab_size": 50261
}

Model has a total of 124442880 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=10, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=1.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=10, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 0
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 8333
  steps: 10  ppl: 12.0588
Data size: 2000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
0 are done!
10 are done!
20 are done!
30 are done!
40 are done!
50 are done!
60 are done!
70 are done!
80 are done!
90 are done!
dev bleu: 5.9, dev EM: 0.0
best bleu updated. saved in ../save/concode/checkpoint-10-5.9
best bleu: 5.9
Saving model checkpoint to ../save/concode/checkpoint-10-5.9
Saving optimizer and scheduler states to ../save/concode/checkpoint-last
  steps: 20  ppl: 4.5782
Data size: 2000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
0 are done!
10 are done!
20 are done!
30 are done!
40 are done!
50 are done!
60 are done!
70 are done!
80 are done!
90 are done!
dev bleu: 3.43, dev EM: 3.0
Saving model checkpoint to ../save/concode/checkpoint-20-3.43
Saving optimizer and scheduler states to ../save/concode/checkpoint-last
  steps: 30  ppl: 3.3994
Data size: 2000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
0 are done!
reload model from ../save/concode/checkpoint-last, resume from 1 epoch
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "../save/concode/checkpoint-last",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "vocab_size": 50261
}

Model has a total of 124442880 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, config_name='../save/concode/checkpoint-last/config.json', data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=100, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=30.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='../save/concode/checkpoint-last', save_steps=1000, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=1, start_step=20, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 29
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 249990
[0, 36667, 12023, 1, 2]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java",
  "_num_labels": 2,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "output_past": true,
  "pad_token_id": 1,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50001
}

Model has a total of 124243200 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=100, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=30.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java', save_steps=1000, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 29
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 249990
[0, 36667, 12023, 1, 2]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java",
  "_num_labels": 2,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "output_past": true,
  "pad_token_id": 1,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50001
}

Model has a total of 124243200 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=100, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=5.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java', save_steps=1000, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 4
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 41665
[0, 36667, 12023, 1, 2]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java",
  "_num_labels": 2,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "output_past": true,
  "pad_token_id": 1,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50001
}

Model has a total of 124243200 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=100, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=1.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java', save_steps=1000, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 0
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 8333
[0, 36667, 12023, 1, 2]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java",
  "_num_labels": 2,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "output_past": true,
  "pad_token_id": 1,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50001
}

Model has a total of 124243200 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=100, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=30.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java', save_steps=1000, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 29
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 249990
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "vocab_size": 50261
}

Model has a total of 124442880 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=100, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=30.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=1000, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 29
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 249990
  steps: 100  ppl: 3.2834
  steps: 200  ppl: 2.3655
  steps: 300  ppl: 2.1339
  steps: 400  ppl: 2.1097
  steps: 500  ppl: 2.0782
  steps: 600  ppl: 1.9682
  steps: 700  ppl: 1.9535
  steps: 800  ppl: 1.9477
  steps: 900  ppl: 1.9657
  steps: 1000  ppl: 1.8963
Data size: 2000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
0 are done!
dev bleu: 10.81, dev EM: 13.0
best bleu updated. saved in ../save/concode/checkpoint-1000-10.81
best bleu: 10.81
Saving model checkpoint to ../save/concode/checkpoint-1000-10.81
Saving optimizer and scheduler states to ../save/concode/checkpoint-last
  steps: 1100  ppl: 1.8876
  steps: 1200  ppl: 1.8901
  steps: 1300  ppl: 1.8491
  steps: 1400  ppl: 1.8885
  steps: 1500  ppl: 1.8745
  steps: 1600  ppl: 1.8236
  steps: 1700  ppl: 1.8612
  steps: 1800  ppl: 1.8295
  steps: 1900  ppl: 1.802
  steps: 2000  ppl: 1.818
Data size: 2000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
0 are done!
dev bleu: 13.06, dev EM: 17.0
best bleu updated. saved in ../save/concode/checkpoint-2000-13.06
best bleu: 13.06
Saving model checkpoint to ../save/concode/checkpoint-2000-13.06
Saving optimizer and scheduler states to ../save/concode/checkpoint-last
  steps: 2100  ppl: 1.7719
  steps: 2200  ppl: 1.8391
  steps: 2300  ppl: 1.8015
  steps: 2400  ppl: 1.809
  steps: 2500  ppl: 1.8406
  steps: 2600  ppl: 1.8343
  steps: 2700  ppl: 1.786
  steps: 2800  ppl: 1.7714
  steps: 2900  ppl: 1.8108
  steps: 3000  ppl: 1.7628
Data size: 2000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
0 are done!
dev bleu: 14.04, dev EM: 18.0
best bleu updated. saved in ../save/concode/checkpoint-3000-14.04
best bleu: 14.04
Saving model checkpoint to ../save/concode/checkpoint-3000-14.04
Saving optimizer and scheduler states to ../save/concode/checkpoint-last
  steps: 3100  ppl: 1.7681
  steps: 3200  ppl: 1.7402
  steps: 3300  ppl: 1.7908
  steps: 3400  ppl: 1.7502
  steps: 3500  ppl: 1.755
  steps: 3600  ppl: 1.7694
  steps: 3700  ppl: 1.7627
  steps: 3800  ppl: 1.7292
  steps: 3900  ppl: 1.74
  steps: 4000  ppl: 1.683
Data size: 2000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
0 are done!
dev bleu: 13.3, dev EM: 18.0
Saving model checkpoint to ../save/concode/checkpoint-4000-13.3
Saving optimizer and scheduler states to ../save/concode/checkpoint-last
  steps: 4100  ppl: 1.7319
  steps: 4200  ppl: 1.71
  steps: 4300  ppl: 1.747
  steps: 4400  ppl: 1.7563
  steps: 4500  ppl: 1.7438
  steps: 4600  ppl: 1.7129
  steps: 4700  ppl: 1.6798
  steps: 4800  ppl: 1.6756
  steps: 4900  ppl: 1.6714
  steps: 5000  ppl: 1.6935
Data size: 2000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
0 are done!
dev bleu: 14.15, dev EM: 17.0
best bleu updated. saved in ../save/concode/checkpoint-5000-14.15
best bleu: 14.15
Saving model checkpoint to ../save/concode/checkpoint-5000-14.15
Saving optimizer and scheduler states to ../save/concode/checkpoint-last
  steps: 5100  ppl: 1.6939
  steps: 5200  ppl: 1.7035
  steps: 5300  ppl: 1.6756
reload model from ../save/concode/checkpoint-last, resume from 1 epoch
[50259, 23748, 995, 50257, 50258]
[0, 36667, 12023, 1, 2]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java",
  "_num_labels": 2,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "output_past": true,
  "pad_token_id": 1,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50001
}

Model has a total of 124243200 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=100, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=30.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java', save_steps=1000, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 29
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 249990
[0, 20760, 232, 1437, 1, 1437, 2]
[50257, 23748, 995, 50260, 50258]
GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50257,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50260,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "vocab_size": 50261
}

Model has a total of 124442880 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=100, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=30.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='gpt2', save_steps=1000, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 29
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 249990
  steps: 100  ppl: 17.4274
  steps: 200  ppl: 3.8988
  steps: 300  ppl: 3.3059
  steps: 400  ppl: 3.2378
  steps: 500  ppl: 3.1172
  steps: 600  ppl: 2.8859
  steps: 700  ppl: 2.8152
  steps: 800  ppl: 2.7788
  steps: 900  ppl: 2.7498
  steps: 1000  ppl: 2.6556
Data size: 2000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
0 are done!
dev bleu: 8.66, dev EM: 7.0
best bleu updated. saved in ../save/concode/checkpoint-1000-8.66
best bleu: 8.66
Saving model checkpoint to ../save/concode/checkpoint-1000-8.66
Saving optimizer and scheduler states to ../save/concode/checkpoint-last
  steps: 1100  ppl: 2.606
  steps: 1200  ppl: 2.6012
  steps: 1300  ppl: 2.5033
  steps: 1400  ppl: 2.5873
  steps: 1500  ppl: 2.5562
  steps: 1600  ppl: 2.47
  steps: 1700  ppl: 2.5436
  steps: 1800  ppl: 2.4606
  steps: 1900  ppl: 2.4281
  steps: 2000  ppl: 2.403
Data size: 2000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
0 are done!
dev bleu: 9.0, dev EM: 8.0
best bleu updated. saved in ../save/concode/checkpoint-2000-9.0
best bleu: 9.0
Saving model checkpoint to ../save/concode/checkpoint-2000-9.0
Saving optimizer and scheduler states to ../save/concode/checkpoint-last
  steps: 2100  ppl: 2.3767
  steps: 2200  ppl: 2.4339
  steps: 2300  ppl: 2.3754
  steps: 2400  ppl: 2.3789
  steps: 2500  ppl: 2.4405
  steps: 2600  ppl: 2.4172
  steps: 2700  ppl: 2.342
  steps: 2800  ppl: 2.2561
  steps: 2900  ppl: 2.3407
  steps: 3000  ppl: 2.2675
Data size: 2000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
0 are done!
dev bleu: 11.06, dev EM: 11.0
best bleu updated. saved in ../save/concode/checkpoint-3000-11.06
best bleu: 11.06
Saving model checkpoint to ../save/concode/checkpoint-3000-11.06
Saving optimizer and scheduler states to ../save/concode/checkpoint-last
  steps: 3100  ppl: 2.2556
  steps: 3200  ppl: 2.2256
  steps: 3300  ppl: 2.3098
  steps: 3400  ppl: 2.2275
  steps: 3500  ppl: 2.2122
  steps: 3600  ppl: 2.2698
  steps: 3700  ppl: 2.2606
  steps: 3800  ppl: 2.1676
  steps: 3900  ppl: 2.1917
  steps: 4000  ppl: 2.1218
Data size: 2000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
0 are done!
dev bleu: 10.43, dev EM: 10.0
Saving model checkpoint to ../save/concode/checkpoint-4000-10.43
Saving optimizer and scheduler states to ../save/concode/checkpoint-last
  steps: 4100  ppl: 2.1815
  steps: 4200  ppl: 2.1585
  steps: 4300  ppl: 2.1905
  steps: 4400  ppl: 2.233
  steps: 4500  ppl: 2.2034
  steps: 4600  ppl: 2.1557
  steps: 4700  ppl: 2.113
  steps: 4800  ppl: 2.0863
  steps: 4900  ppl: 2.0843
  steps: 5000  ppl: 2.1043
Data size: 2000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
0 are done!
dev bleu: 16.91, dev EM: 10.0
best bleu updated. saved in ../save/concode/checkpoint-5000-16.91
best bleu: 16.91
Saving model checkpoint to ../save/concode/checkpoint-5000-16.91
Saving optimizer and scheduler states to ../save/concode/checkpoint-last
  steps: 5100  ppl: 2.1201
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "use_cache": true,
  "vocab_size": 50261
}

Model has a total of 124442880 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=100, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=30.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=1000, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 29
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 249990
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "use_cache": true,
  "vocab_size": 50261
}

Model has a total of 124442880 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=100, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=30.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=1000, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "use_cache": true,
  "vocab_size": 50261
}

[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "use_cache": true,
  "vocab_size": 50261
}

Model has a total of 680958141 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=100, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=30.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=1000, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 29
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 249990
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "use_cache": true,
  "vocab_size": 50261
}

Model has a total of 680958141 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=100, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=30.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=1000, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 29
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 249990
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "use_cache": true,
  "vocab_size": 50261
}

Model has a total of 680958141 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=100, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=30.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=1000, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 29
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 249990
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "use_cache": true,
  "vocab_size": 50261
}

Model has a total of 379643141 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=100, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=30.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=1000, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 29
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 249990
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "use_cache": true,
  "vocab_size": 50261
}

Model has a total of 379643141 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=100, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=30.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=6, per_gpu_train_batch_size=3, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=1000, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 29
  Instantaneous batch size per GPU = 3
  Total train batch size (w. parallel, distributed & accumulation) = 6
  Gradient Accumulation steps = 2
  Total optimization steps = 499980
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "use_cache": true,
  "vocab_size": 50261
}

Model has a total of 680958141 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=100, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=30.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=6, per_gpu_train_batch_size=3, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=1000, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 29
  Instantaneous batch size per GPU = 3
  Total train batch size (w. parallel, distributed & accumulation) = 6
  Gradient Accumulation steps = 2
  Total optimization steps = 499980
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "use_cache": true,
  "vocab_size": 50261
}

Model has a total of 680958141 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=100, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=30.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=4, per_gpu_train_batch_size=2, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=1000, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 30
  Instantaneous batch size per GPU = 2
  Total train batch size (w. parallel, distributed & accumulation) = 4
  Gradient Accumulation steps = 2
  Total optimization steps = 750000
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "use_cache": true,
  "vocab_size": 50261
}

Model has a total of 680958141 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=100, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=30.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=1, per_gpu_train_batch_size=1, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=1000, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 30
  Instantaneous batch size per GPU = 1
  Total train batch size (w. parallel, distributed & accumulation) = 2
  Gradient Accumulation steps = 2
  Total optimization steps = 1500000
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "use_cache": true,
  "vocab_size": 50261
}

Model has a total of 680958141 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=100, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=30.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=24, per_gpu_train_batch_size=12, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=1000, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 29
  Instantaneous batch size per GPU = 12
  Total train batch size (w. parallel, distributed & accumulation) = 24
  Gradient Accumulation steps = 2
  Total optimization steps = 124980
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "use_cache": true,
  "vocab_size": 50261
}

Model has a total of 680958141 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=100, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=30.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=1000, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 29
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 249990
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "use_cache": true,
  "vocab_size": 50261
}

Model has a total of 680958141 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=100, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=30.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=1000, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 29
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 249990
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "use_cache": true,
  "vocab_size": 50261
}

Model has a total of 680958141 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=100, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=30.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=6, per_gpu_train_batch_size=3, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=1000, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 29
  Instantaneous batch size per GPU = 3
  Total train batch size (w. parallel, distributed & accumulation) = 6
  Gradient Accumulation steps = 2
  Total optimization steps = 499980
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "use_cache": true,
  "vocab_size": 50261
}

Model has a total of 680958141 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=100, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=30.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=6, per_gpu_train_batch_size=3, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=1000, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 29
  Instantaneous batch size per GPU = 3
  Total train batch size (w. parallel, distributed & accumulation) = 6
  Gradient Accumulation steps = 2
  Total optimization steps = 499980
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "use_cache": true,
  "vocab_size": 50261
}

Model has a total of 680958141 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=100, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=30.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=1000, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 29
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 249990
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "use_cache": true,
  "vocab_size": 50261
}

Model has a total of 680958141 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=100, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=30.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=1000, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 29
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 249990
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "use_cache": true,
  "vocab_size": 50261
}

Model has a total of 680958141 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=100, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=30.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=6, per_gpu_train_batch_size=3, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=1000, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 29
  Instantaneous batch size per GPU = 3
  Total train batch size (w. parallel, distributed & accumulation) = 6
  Gradient Accumulation steps = 2
  Total optimization steps = 499980
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "use_cache": true,
  "vocab_size": 50261
}

Model has a total of 680958141 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=100, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=30.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=24, per_gpu_train_batch_size=12, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=1000, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 29
  Instantaneous batch size per GPU = 12
  Total train batch size (w. parallel, distributed & accumulation) = 24
  Gradient Accumulation steps = 2
  Total optimization steps = 124980
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "use_cache": true,
  "vocab_size": 50261
}

Model has a total of 680958141 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=100, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=30.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=48, per_gpu_train_batch_size=24, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=1000, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 29
  Instantaneous batch size per GPU = 24
  Total train batch size (w. parallel, distributed & accumulation) = 48
  Gradient Accumulation steps = 2
  Total optimization steps = 62490
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "use_cache": true,
  "vocab_size": 50261
}

Model has a total of 680958141 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=100, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=30.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=24, per_gpu_train_batch_size=12, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=1000, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 29
  Instantaneous batch size per GPU = 12
  Total train batch size (w. parallel, distributed & accumulation) = 24
  Gradient Accumulation steps = 2
  Total optimization steps = 124980
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "use_cache": true,
  "vocab_size": 50261
}

Model has a total of 680958141 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=100, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=30.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=24, per_gpu_train_batch_size=12, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=1000, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 29
  Instantaneous batch size per GPU = 12
  Total train batch size (w. parallel, distributed & accumulation) = 24
  Gradient Accumulation steps = 2
  Total optimization steps = 124980
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "use_cache": true,
  "vocab_size": 50261
}

[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "use_cache": true,
  "vocab_size": 50261
}

[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "use_cache": true,
  "vocab_size": 50261
}

[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "use_cache": true,
  "vocab_size": 50261
}

Model has a total of 680958141 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=100, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=30.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=24, per_gpu_train_batch_size=12, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=1000, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 29
  Instantaneous batch size per GPU = 12
  Total train batch size (w. parallel, distributed & accumulation) = 24
  Gradient Accumulation steps = 2
  Total optimization steps = 124980
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "use_cache": true,
  "vocab_size": 50261
}

Model has a total of 680958141 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=100, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=30.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=24, per_gpu_train_batch_size=12, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=1000, save_toptimizer_grouped_parametersotal_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "use_cache": true,
  "vocab_size": 50261
}

Model has a total of 680958141 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=100, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=30.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=1000, save_toptimizer_grouped_parametersotal_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "use_cache": true,
  "vocab_size": 50261
}

Model has a total of 680958141 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=100, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=30.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=1000, save_toptimizer_grouped_parametersotal_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Creating features from dataset file at ../dataset/concode/train.json
Data size: 90000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
Rank 0 Training 90000 token, 90000 samples
Saving features into cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "use_cache": true,
  "vocab_size": 50261
}

Model has a total of 680958141 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=100, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=30.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=1000, save_toptimizer_grouped_parametersotal_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "use_cache": true,
  "vocab_size": 50261
}

Model has a total of 680958141 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=100, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=30.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=1000, save_toptimizer_grouped_parametersotal_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 90000
  Num epoch = 30
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 225000
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "use_cache": true,
  "vocab_size": 50261
}

Model has a total of 680958141 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=100, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=30.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=1000, save_toptimizer_grouped_parametersotal_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 90000
  Num epoch = 30
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 225000
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "use_cache": true,
  "vocab_size": 50261
}

Model has a total of 680958141 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=100, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=30.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=6, per_gpu_train_batch_size=3, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=1000, save_toptimizer_grouped_parametersotal_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 90000
  Num epoch = 30
  Instantaneous batch size per GPU = 3
  Total train batch size (w. parallel, distributed & accumulation) = 6
  Gradient Accumulation steps = 2
  Total optimization steps = 450000
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "use_cache": true,
  "vocab_size": 50261
}

Model has a total of 680958141 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=100, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=30.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=4, per_gpu_train_batch_size=2, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=1000, save_toptimizer_grouped_parametersotal_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 90000
  Num epoch = 30
  Instantaneous batch size per GPU = 2
  Total train batch size (w. parallel, distributed & accumulation) = 4
  Gradient Accumulation steps = 2
  Total optimization steps = 675000
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "use_cache": true,
  "vocab_size": 50261
}

Model has a total of 680958141 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=256, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=100, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=30.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=1000, save_toptimizer_grouped_parametersotal_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Creating features from dataset file at ../dataset/concode/train.json
Data size: 90000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
Rank 0 Training 90000 token, 90000 samples
Saving features into cached file ../save/concode/train_blocksize_256_wordsize_1_rank_0
***** Running training *****
  Num examples = 90000
  Num epoch = 30
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 225000
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "use_cache": true,
  "vocab_size": 50261
}

Model has a total of 680958141 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=1024, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=100, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=30.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=1000, save_toptimizer_grouped_parametersotal_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Creating features from dataset file at ../dataset/concode/train.json
Data size: 90000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
Rank 0 Training 90000 token, 90000 samples
Saving features into cached file ../save/concode/train_blocksize_1024_wordsize_1_rank_0
***** Running training *****
  Num examples = 90000
  Num epoch = 30
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 225000
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "use_cache": true,
  "vocab_size": 50261
}

Model has a total of 680958141 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=100, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=30.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=1000, save_toptimizer_grouped_parametersotal_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "use_cache": true,
  "vocab_size": 50261
}

Model has a total of 680958141 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=100, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=30.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=1000, save_toptimizer_grouped_parametersotal_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 90000
  Num epoch = 30
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 225000
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "use_cache": true,
  "vocab_size": 50261
}

Model has a total of 680958141 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=100, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=30.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=1000, save_toptimizer_grouped_parametersotal_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Creating features from dataset file at ../dataset/concode/train.json
Data size: 50000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
Rank 0 Training 50000 token, 50000 samples
Saving features into cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 50000
  Num epoch = 29
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 124980
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "use_cache": true,
  "vocab_size": 50261
}

Model has a total of 680958141 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=256, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=100, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=30.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=1000, save_toptimizer_grouped_parametersotal_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Creating features from dataset file at ../dataset/concode/train.json
Data size: 50000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
Rank 0 Training 50000 token, 50000 samples
Saving features into cached file ../save/concode/train_blocksize_256_wordsize_1_rank_0
***** Running training *****
  Num examples = 50000
  Num epoch = 29
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 124980
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "use_cache": true,
  "vocab_size": 50261
}

Model has a total of 680958141 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=256, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=100, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=30.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=6, per_gpu_train_batch_size=3, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=1000, save_toptimizer_grouped_parametersotal_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_256_wordsize_1_rank_0
***** Running training *****
  Num examples = 50000
  Num epoch = 29
  Instantaneous batch size per GPU = 3
  Total train batch size (w. parallel, distributed & accumulation) = 6
  Gradient Accumulation steps = 2
  Total optimization steps = 249990
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "use_cache": true,
  "vocab_size": 50261
}

Model has a total of 680958141 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=256, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=100, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=30.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=6, per_gpu_train_batch_size=3, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=1000, save_toptimizer_grouped_parametersotal_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_256_wordsize_1_rank_0
***** Running training *****
  Num examples = 50000
  Num epoch = 29
  Instantaneous batch size per GPU = 3
  Total train batch size (w. parallel, distributed & accumulation) = 6
  Gradient Accumulation steps = 2
  Total optimization steps = 249990
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "use_cache": true,
  "vocab_size": 50261
}

Model has a total of 680958141 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=256, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=100, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=30.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=1, per_gpu_train_batch_size=1, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=1000, save_toptimizer_grouped_parametersotal_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_256_wordsize_1_rank_0
***** Running training *****
  Num examples = 50000
  Num epoch = 30
  Instantaneous batch size per GPU = 1
  Total train batch size (w. parallel, distributed & accumulation) = 2
  Gradient Accumulation steps = 2
  Total optimization steps = 750000
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "use_cache": true,
  "vocab_size": 50261
}

Model has a total of 680958141 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=256, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=10, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=100, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=30.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=1, per_gpu_train_batch_size=1, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=1000, save_toptimizer_grouped_parametersotal_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_256_wordsize_1_rank_0
***** Running training *****
  Num examples = 50000
  Num epoch = 30
  Instantaneous batch size per GPU = 1
  Total train batch size (w. parallel, distributed & accumulation) = 10
  Gradient Accumulation steps = 10
  Total optimization steps = 150000
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "use_cache": true,
  "vocab_size": 50261
}

Model has a total of 680958141 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=128, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=10, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=100, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=30.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=1, per_gpu_train_batch_size=1, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=1000, save_toptimizer_grouped_parametersotal_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Creating features from dataset file at ../dataset/concode/train.json
Data size: 50000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
Rank 0 Training 50000 token, 50000 samples
Saving features into cached file ../save/concode/train_blocksize_128_wordsize_1_rank_0
***** Running training *****
  Num examples = 50000
  Num epoch = 30
  Instantaneous batch size per GPU = 1
  Total train batch size (w. parallel, distributed & accumulation) = 10
  Gradient Accumulation steps = 10
  Total optimization steps = 150000
[50257, 23748, 995, 50260, 50258]
GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50257,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50260,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "use_cache": true,
  "vocab_size": 50261
}

Model has a total of 680958141 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=100, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=30.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='gpt2', save_steps=1000, save_toptimizer_grouped_parametersotal_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 50000
  Num epoch = 29
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 124980
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "use_cache": true,
  "vocab_size": 50261
}

Model has a total of 680958141 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=100, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=30.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=1000, save_toptimizer_grouped_parametersotal_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 50000
  Num epoch = 29
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 124980
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "use_cache": true,
  "vocab_size": 50261
}

Model has a total of 379643141 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=100, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=30.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=6, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=1000, save_toptimizer_grouped_parametersotal_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 50000
  Num epoch = 29
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 124980
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "use_cache": true,
  "vocab_size": 50261
}

Model has a total of 328613141 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=10, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=100, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=30.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=6, per_gpu_train_batch_size=3, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=1000, save_toptimizer_grouped_parametersotal_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 50000
  Num epoch = 29
  Instantaneous batch size per GPU = 3
  Total train batch size (w. parallel, distributed & accumulation) = 30
  Gradient Accumulation steps = 10
  Total optimization steps = 49980
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "use_cache": true,
  "vocab_size": 50261
}

Model has a total of 328613141 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=100, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=30.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=6, per_gpu_train_batch_size=3, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=1000, save_toptimizer_grouped_parametersotal_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 50000
  Num epoch = 29
  Instantaneous batch size per GPU = 3
  Total train batch size (w. parallel, distributed & accumulation) = 6
  Gradient Accumulation steps = 2
  Total optimization steps = 249990
  steps: 100  ppl: 72.7285
  steps: 200  ppl: 14.8806
  steps: 300  ppl: 11.2812
  steps: 400  ppl: 8.137
  steps: 500  ppl: 7.8291
  steps: 600  ppl: 6.2773
  steps: 700  ppl: 6.4587
  steps: 800  ppl: 5.2072
  steps: 900  ppl: 5.2957
  steps: 1000  ppl: 4.4419
Data size: 2000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
[50259, 23748, 995, 50257, 50258]
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "use_cache": true,
  "vocab_size": 50261
}

Model has a total of 328613141 trainable parameters
Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=100, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=30.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=6, per_gpu_train_batch_size=3, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=1000, save_toptimizer_grouped_parametersotal_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)
Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 50000
  Num epoch = 29
  Instantaneous batch size per GPU = 3
  Total train batch size (w. parallel, distributed & accumulation) = 6
  Gradient Accumulation steps = 2
  Total optimization steps = 249990
  steps: 100  ppl: 72.7285
  steps: 200  ppl: 14.8806
  steps: 300  ppl: 11.2812
  steps: 400  ppl: 8.137
  steps: 500  ppl: 7.8291
  steps: 600  ppl: 6.2773
  steps: 700  ppl: 6.4587
  steps: 800  ppl: 5.2072
  steps: 900  ppl: 5.2957
  steps: 1000  ppl: 4.4419
Data size: 2000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
