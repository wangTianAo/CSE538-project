{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Tianao Wang.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyP+hK4rAsywCKSRoSCdAZTF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KbfxbsZxUAuc","executionInfo":{"status":"ok","timestamp":1606775984382,"user_tz":-480,"elapsed":16037,"user":{"displayName":"Tianao Wang","photoUrl":"","userId":"08584333101059820215"}},"outputId":"1a719926-7cc7-4e4b-ba18-e9dc007dfd7f"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bIwYEGfqUR4K","executionInfo":{"status":"ok","timestamp":1606775987224,"user_tz":-480,"elapsed":7102,"user":{"displayName":"Tianao Wang","photoUrl":"","userId":"08584333101059820215"}},"outputId":"1892ffaa-4577-4952-9bc9-89abae40acb1"},"source":["cd '/content/gdrive/My Drive/CSE538/project/text-to-code/code/'"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/CSE538/project/text-to-code/code\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E1Om9EpzVeaj","executionInfo":{"status":"ok","timestamp":1606236714158,"user_tz":-480,"elapsed":1079,"user":{"displayName":"Tianao Wang","photoUrl":"","userId":"08584333101059820215"}},"outputId":"14ad6c77-f370-488b-f7e8-919bd5020aa8"},"source":["ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["beam.py  bleu.py  dataset.py  \u001b[0m\u001b[01;34m__pycache__\u001b[0m/  run.py  text2code_concode.log\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"XE4GCOnIK1tr","executionInfo":{"status":"ok","timestamp":1606744375759,"user_tz":-480,"elapsed":5118,"user":{"displayName":"Tianao Wang","photoUrl":"","userId":"08584333101059820215"}},"outputId":"c0ba7aa0-6264-4ddf-b773-1882588925f7"},"source":["import torch\n","torch.__version__"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'1.7.0+cu101'"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QMkB4uMdJCQp","executionInfo":{"status":"ok","timestamp":1606776134758,"user_tz":-480,"elapsed":150662,"user":{"displayName":"Tianao Wang","photoUrl":"","userId":"08584333101059820215"}},"outputId":"819f9fee-04b3-4c73-ace0-8608f0ccfe64"},"source":["pip install torch==1.4.0 torchvision==0.5.0"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting torch==1.4.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/19/4804aea17cd136f1705a5e98a00618cb8f6ccc375ad8bfa437408e09d058/torch-1.4.0-cp36-cp36m-manylinux1_x86_64.whl (753.4MB)\n","\u001b[K     |████████████████████████████████| 753.4MB 20kB/s \n","\u001b[?25hCollecting torchvision==0.5.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/90/6141bf41f5655c78e24f40f710fdd4f8a8aff6c8b7c6f0328240f649bdbe/torchvision-0.5.0-cp36-cp36m-manylinux1_x86_64.whl (4.0MB)\n","\u001b[K     |████████████████████████████████| 4.0MB 46.2MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0) (1.18.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0) (1.15.0)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0) (7.0.0)\n","Installing collected packages: torch, torchvision\n","  Found existing installation: torch 1.7.0+cu101\n","    Uninstalling torch-1.7.0+cu101:\n","      Successfully uninstalled torch-1.7.0+cu101\n","  Found existing installation: torchvision 0.8.1+cu101\n","    Uninstalling torchvision-0.8.1+cu101:\n","      Successfully uninstalled torchvision-0.8.1+cu101\n","Successfully installed torch-1.4.0 torchvision-0.5.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aTNDfkHPWItz","executionInfo":{"status":"ok","timestamp":1606776140743,"user_tz":-480,"elapsed":155091,"user":{"displayName":"Tianao Wang","photoUrl":"","userId":"08584333101059820215"}},"outputId":"b827c4f8-86a3-45c2-8cb3-0cc972279383"},"source":["!pip install transformers"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/84/7bc03215279f603125d844bf81c3fb3f2d50fe8e511546eb4897e4be2067/transformers-4.0.0-py3-none-any.whl (1.4MB)\n","\u001b[K     |████████████████████████████████| 1.4MB 8.4MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Collecting tokenizers==0.9.4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n","\u001b[K     |████████████████████████████████| 2.9MB 29.3MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 51.3MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.11.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=5ed67adbaf610dd7de555ca226602fd0e944ec8aa08938cdbe4d9ffe9e8a8368\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sacremoses, transformers\n","Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.0.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aWTnNyOdY5pA","executionInfo":{"status":"ok","timestamp":1606775397714,"user_tz":-480,"elapsed":766,"user":{"displayName":"Tianao Wang","photoUrl":"","userId":"08584333101059820215"}},"outputId":"3e2d22a1-20ed-409b-eee2-70a41e63f723"},"source":["ls"],"execution_count":65,"outputs":[{"output_type":"stream","text":["dev.json  test1.json  test.json  train_7500.json  train.json\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0cLEOPJARQO7","executionInfo":{"status":"ok","timestamp":1606775344034,"user_tz":-480,"elapsed":1211,"user":{"displayName":"Tianao Wang","photoUrl":"","userId":"08584333101059820215"}}},"source":["!head -50000 train.json >train_7500.json"],"execution_count":55,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EWYTLzY2aA-q","executionInfo":{"status":"ok","timestamp":1606776275722,"user_tz":-480,"elapsed":791,"user":{"displayName":"Tianao Wang","photoUrl":"","userId":"08584333101059820215"}},"outputId":"353aa05b-8643-4d05-aeb1-3b4f7ea2a200"},"source":["! /opt/bin/nvidia-smi"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Mon Nov 30 22:44:21 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   53C    P8    11W /  70W |      0MiB / 15079MiB |      0%      Default |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                       GPU Memory |\n","|  GPU       PID   Type   Process name                             Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d8KsnMJ0Q9bj","executionInfo":{"status":"ok","timestamp":1606775386596,"user_tz":-480,"elapsed":793,"user":{"displayName":"Tianao Wang","photoUrl":"","userId":"08584333101059820215"}},"outputId":"95d27c42-9fce-425a-b559-ef19114011a8"},"source":["cd '/content/gdrive/My Drive/CSE538/project/text-to-code/dataset/concode'"],"execution_count":62,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/CSE538/project/text-to-code/dataset/concode\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7Ii2j5DY8LCo"},"source":["# train"]},{"cell_type":"code","metadata":{"id":"Ij0abAYUWalw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606782262264,"user_tz":-480,"elapsed":1959166,"user":{"displayName":"Tianao Wang","photoUrl":"","userId":"08584333101059820215"}},"outputId":"60b9b486-e1c0-483d-802a-b8dad5031f1f"},"source":["!python -m torch.distributed.launch --nproc_per_node=1 run.py \\\n","        --data_dir=../dataset/concode \\\n","        --langs=java \\\n","        --output_dir=../save/concode \\\n","        --pretrain_dir=microsoft/CodeGPT-small-java-adaptedGPT2 \\\n","        --log_file=text2code_concode.log \\\n","        --model_type=gpt2 \\\n","        --block_size=512 \\\n","        --do_train \\\n","        --node_index 0 \\\n","        --gpu_per_node 1 \\\n","        --learning_rate=5e-5 \\\n","        --weight_decay=0.01 \\\n","        --evaluate_during_training \\\n","        --per_gpu_train_batch_size=3 \\\n","        --per_gpu_eval_batch_size=6 \\\n","        --gradient_accumulation_steps=2 \\\n","        --num_train_epochs=30 \\\n","        --logging_steps=100 \\\n","        --save_steps=1000 \\\n","        --overwrite_output_dir \\\n","        --seed=42"],"execution_count":17,"outputs":[{"output_type":"stream","text":["2020-11-30 23:51:31.231443: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n","local_rank: 0, node_index: 0, gpu_per_node: 1\n","11/30/2020 23:51:33 - WARNING - __main__ -   Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False, world size: 1\n","Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n","11/30/2020 23:51:34 - INFO - __main__ -   [50259, 23748, 995, 50257, 50258]\n","11/30/2020 23:51:39 - INFO - __main__ -   GPT2Config {\n","  \"_name_or_path\": \"microsoft/CodeGPT-small-java-adaptedGPT2\",\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"bos_token_id\": 50259,\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 50258,\n","  \"gradient_checkpointing\": false,\n","  \"initializer_range\": 0.02,\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 768,\n","  \"n_head\": 12,\n","  \"n_inner\": null,\n","  \"n_layer\": 12,\n","  \"n_positions\": 1024,\n","  \"pad_token_id\": 50257,\n","  \"resid_pdrop\": 0.1,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"use_cache\": true,\n","  \"vocab_size\": 50261\n","}\n","\n","123\n","Linear(in_features=768, out_features=50261, bias=False)\n","11/30/2020 23:51:41 - INFO - __main__ -   Model has a total of 328613141 trainable parameters\n","11/30/2020 23:51:46 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda', index=0), do_eval=False, do_infer=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_per_node=1, gradient_accumulation_steps=2, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=0, log_file='text2code_concode.log', logging_steps=100, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=0, num_train_epochs=30.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=6, per_gpu_train_batch_size=3, pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', save_steps=1000, save_toptimizer_grouped_parametersotal_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.01)\n","11/30/2020 23:51:46 - WARNING - __main__ -   Loading features from cached file ../save/concode/train_blocksize_512_wordsize_1_rank_0\n","11/30/2020 23:51:48 - INFO - __main__ -   ***** Running training *****\n","11/30/2020 23:51:48 - INFO - __main__ -     Num examples = 50000\n","11/30/2020 23:51:48 - INFO - __main__ -     Num epoch = 29\n","11/30/2020 23:51:48 - INFO - __main__ -     Instantaneous batch size per GPU = 3\n","11/30/2020 23:51:48 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 6\n","11/30/2020 23:51:48 - INFO - __main__ -     Gradient Accumulation steps = 2\n","11/30/2020 23:51:48 - INFO - __main__ -     Total optimization steps = 249990\n","run.py:168: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  attn_mask = torch.tensor(token_labels.clone().detach() != 0, dtype=torch.uint8, device=args.device)\n","run.py:169: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  loss_mask = torch.tensor(token_labels.clone().detach() == 2, dtype=torch.uint8, device=args.device)\n","11/30/2020 23:54:57 - INFO - __main__ -     steps: 100  ppl: 72.7285\n","11/30/2020 23:58:11 - INFO - __main__ -     steps: 200  ppl: 14.8806\n","12/01/2020 00:01:25 - INFO - __main__ -     steps: 300  ppl: 11.2812\n","12/01/2020 00:04:38 - INFO - __main__ -     steps: 400  ppl: 8.137\n","12/01/2020 00:07:52 - INFO - __main__ -     steps: 500  ppl: 7.8291\n","12/01/2020 00:11:06 - INFO - __main__ -     steps: 600  ppl: 6.2773\n","12/01/2020 00:14:20 - INFO - __main__ -     steps: 700  ppl: 6.4587\n","12/01/2020 00:17:34 - INFO - __main__ -     steps: 800  ppl: 5.2072\n","12/01/2020 00:20:47 - INFO - __main__ -     steps: 900  ppl: 5.2957\n","12/01/2020 00:24:01 - INFO - __main__ -     steps: 1000  ppl: 4.4419\n","12/01/2020 00:24:01 - INFO - __main__ -   Data size: 2000\n","12/01/2020 00:24:01 - WARNING - __main__ -   Rank 0, load 0\n","12/01/2020 00:24:02 - WARNING - __main__ -   Rank 0, load 10\n","12/01/2020 00:24:02 - WARNING - __main__ -   Rank 0, load 20\n","12/01/2020 00:24:02 - WARNING - __main__ -   Rank 0, load 30\n","12/01/2020 00:24:03 - WARNING - __main__ -   Rank 0, load 40\n","12/01/2020 00:24:03 - WARNING - __main__ -   Rank 0, load 50\n","12/01/2020 00:24:04 - WARNING - __main__ -   Rank 0, load 60\n","12/01/2020 00:24:04 - WARNING - __main__ -   Rank 0, load 70\n","12/01/2020 00:24:04 - WARNING - __main__ -   Rank 0, load 80\n","12/01/2020 00:24:05 - WARNING - __main__ -   Rank 0, load 90\n","Traceback (most recent call last):\n","  File \"run.py\", line 647, in <module>\n","    main()\n","  File \"run.py\", line 634, in main\n","    global_step, tr_loss = train(args, train_dataset, model, tokenizer, fh, pool)\n","  File \"run.py\", line 224, in train\n","    dev_bleu, dev_EM = eval_bleu(args, model, tokenizer, file_type='dev', num=100)\n","  File \"run.py\", line 371, in eval_bleu\n","    transformer_outputs = model(input_ids, past=past_hidden)\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 532, in __call__\n","    result = self.forward(*input, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/distributed.py\", line 447, in forward\n","    output = self.module(*inputs[0], **kwargs[0])\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 532, in __call__\n","    result = self.forward(*input, **kwargs)\n","TypeError: forward() got an unexpected keyword argument 'past'\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n","    \"__main__\", mod_spec)\n","  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n","    exec(code, run_globals)\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/distributed/launch.py\", line 263, in <module>\n","    main()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/distributed/launch.py\", line 259, in main\n","    cmd=cmd)\n","subprocess.CalledProcessError: Command '['/usr/bin/python3', '-u', 'run.py', '--local_rank=0', '--data_dir=../dataset/concode', '--langs=java', '--output_dir=../save/concode', '--pretrain_dir=microsoft/CodeGPT-small-java-adaptedGPT2', '--log_file=text2code_concode.log', '--model_type=gpt2', '--block_size=512', '--do_train', '--node_index', '0', '--gpu_per_node', '1', '--learning_rate=5e-5', '--weight_decay=0.01', '--evaluate_during_training', '--per_gpu_train_batch_size=3', '--per_gpu_eval_batch_size=6', '--gradient_accumulation_steps=2', '--num_train_epochs=30', '--logging_steps=100', '--save_steps=1000', '--overwrite_output_dir', '--seed=42']' returned non-zero exit status 1.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"sYLBI3-x8N3b"},"source":["# Evaluation"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"om9_611ZOQXz","executionInfo":{"status":"ok","timestamp":1606757968307,"user_tz":-480,"elapsed":2381788,"user":{"displayName":"Tianao Wang","photoUrl":"","userId":"08584333101059820215"}},"outputId":"a3886f08-d805-431c-90c3-84562fbad4df"},"source":["!python -u run.py \\\n","        --data_dir=../dataset/concode \\\n","        --langs=java \\\n","        --output_dir=../save/concode \\\n","        --pretrain_dir=../save/concode/checkpoint-5000-16.91 \\\n","        --log_file=text2code_concode_eval.log \\\n","        --model_type=gpt2 \\\n","        --block_size=512 \\\n","        --do_eval \\\n","        --logging_steps=100 \\\n","        --seed=42"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2020-11-30 16:59:39.000400: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n","local_rank: -1, node_index: -1, gpu_per_node: -1\n","11/30/2020 16:59:44 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False, world size: 1\n","11/30/2020 16:59:44 - INFO - __main__ -   [50257, 23748, 995, 50260, 50258]\n","11/30/2020 16:59:49 - INFO - __main__ -   GPT2Config {\n","  \"_name_or_path\": \"../save/concode/checkpoint-5000-16.91\",\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"bos_token_id\": 50257,\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 50258,\n","  \"gradient_checkpointing\": false,\n","  \"initializer_range\": 0.02,\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 768,\n","  \"n_head\": 12,\n","  \"n_inner\": null,\n","  \"n_layer\": 12,\n","  \"n_positions\": 1024,\n","  \"pad_token_id\": 50260,\n","  \"resid_pdrop\": 0.1,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"vocab_size\": 50261\n","}\n","\n","11/30/2020 16:59:49 - INFO - __main__ -   Model has a total of 124442880 trainable parameters\n","11/30/2020 16:59:49 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir='', config_dir=None, data_dir='../dataset/concode', device=device(type='cuda'), do_eval=True, do_infer=False, do_lower_case=False, do_train=False, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gpu_per_node=-1, gradient_accumulation_steps=1, langs='java', learning_rate=5e-05, load_name='pretrained', local_rank=-1, log_file='text2code_concode_eval.log', logging_steps=100, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_type='gpt2', n_gpu=1, no_cuda=False, node_index=-1, num_train_epochs=1.0, output_dir='../save/concode', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=4, per_gpu_train_batch_size=2, pretrain_dir='../save/concode/checkpoint-5000-16.91', save_steps=50, save_total_limit=None, seed=42, server_ip='', server_port='', start_epoch=0, start_step=0, tensorboard_dir=None, tokenizer_dir=None, warmup_steps=0, weight_decay=0.0)\n","11/30/2020 16:59:49 - INFO - __main__ -   Data size: 2000\n","11/30/2020 16:59:49 - WARNING - __main__ -   Rank 0, load 0\n","11/30/2020 16:59:49 - WARNING - __main__ -   Rank 0, load 10\n","11/30/2020 16:59:50 - WARNING - __main__ -   Rank 0, load 20\n","11/30/2020 16:59:50 - WARNING - __main__ -   Rank 0, load 30\n","11/30/2020 16:59:50 - WARNING - __main__ -   Rank 0, load 40\n","11/30/2020 16:59:51 - WARNING - __main__ -   Rank 0, load 50\n","11/30/2020 16:59:51 - WARNING - __main__ -   Rank 0, load 60\n","11/30/2020 16:59:51 - WARNING - __main__ -   Rank 0, load 70\n","11/30/2020 16:59:52 - WARNING - __main__ -   Rank 0, load 80\n","11/30/2020 16:59:52 - WARNING - __main__ -   Rank 0, load 90\n","/usr/local/lib/python3.6/dist-packages/transformers/modeling_gpt2.py:768: FutureWarning: The `past` argument is deprecated and will be removed in a future version, use `past_key_values` instead.\n","  FutureWarning,\n","11/30/2020 17:00:00 - INFO - __main__ -   0 are done!\n","11/30/2020 17:01:58 - INFO - __main__ -   100 are done!\n","11/30/2020 17:03:51 - INFO - __main__ -   200 are done!\n","11/30/2020 17:05:54 - INFO - __main__ -   300 are done!\n","11/30/2020 17:07:59 - INFO - __main__ -   400 are done!\n","11/30/2020 17:09:58 - INFO - __main__ -   500 are done!\n","11/30/2020 17:12:08 - INFO - __main__ -   600 are done!\n","11/30/2020 17:13:56 - INFO - __main__ -   700 are done!\n","11/30/2020 17:16:01 - INFO - __main__ -   800 are done!\n","11/30/2020 17:18:11 - INFO - __main__ -   900 are done!\n","11/30/2020 17:19:57 - INFO - __main__ -   1000 are done!\n","11/30/2020 17:21:45 - INFO - __main__ -   1100 are done!\n","11/30/2020 17:23:29 - INFO - __main__ -   1200 are done!\n","11/30/2020 17:25:24 - INFO - __main__ -   1300 are done!\n","11/30/2020 17:27:27 - INFO - __main__ -   1400 are done!\n","11/30/2020 17:29:29 - INFO - __main__ -   1500 are done!\n","11/30/2020 17:31:15 - INFO - __main__ -   1600 are done!\n","11/30/2020 17:33:07 - INFO - __main__ -   1700 are done!\n","11/30/2020 17:35:10 - INFO - __main__ -   1800 are done!\n","11/30/2020 17:37:15 - INFO - __main__ -   1900 are done!\n","11/30/2020 17:39:12 - INFO - __main__ -   dev bleu: 15.86, dev EM: 12.4\n"],"name":"stdout"}]}]}